{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Day15- tfidf作業.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAAMPtZUt1hJ"
      },
      "source": [
        "## 作業目標：搭建一個TFIDF 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhbSBm5Pt1hT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15_wRXHzt1hV"
      },
      "source": [
        "#### Reference:https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5nq4iWt1hW"
      },
      "source": [
        "import nltk\n",
        "documentA = 'the man went out for a walk'\n",
        "documentB = 'the children sat around the fire'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZv2uKs0t1hX"
      },
      "source": [
        "## 首先我們做tokenize，並取出所有文件中的單詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoarSJEMKWE_",
        "outputId": "d6d557e9-8fad-4138-f383-b3f9fb0b640e"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vqsO-l9t1hY"
      },
      "source": [
        "tokenize_A = nltk.word_tokenize(documentA)\n",
        "tokenize_B = nltk.word_tokenize(documentB)\n",
        "\n",
        "# set.union(set1, set2...):set2 -- 可選，其他要合併的集合，可以多個，多個使用逗號隔開。\n",
        "uniqueWords = set(tokenize_A).union(set(tokenize_B)) ##所有文件中的單詞"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bARrapT_t1hY"
      },
      "source": [
        "## 計算每個文件中，所有uniqueWords出現的次數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARXRaBOVt1hZ"
      },
      "source": [
        "#http://tw.gitbook.net/python/dictionary_fromkeys.html\n",
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in tokenize_A:\n",
        "    numOfWordsA[word] += 1\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in tokenize_B:\n",
        "    numOfWordsB[word] += 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JETt_9wCt1ha",
        "outputId": "846fa53e-034d-4aae-a862-6f107dd586a6"
      },
      "source": [
        "numOfWordsA"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'around': 0,\n",
              " 'children': 0,\n",
              " 'fire': 0,\n",
              " 'for': 1,\n",
              " 'man': 1,\n",
              " 'out': 1,\n",
              " 'sat': 0,\n",
              " 'the': 1,\n",
              " 'walk': 1,\n",
              " 'went': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TycnT5Rnt1hc",
        "outputId": "6f79666b-55bb-4d58-95d7-842b37054fed"
      },
      "source": [
        "numOfWordsB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0,\n",
              " 'out': 0,\n",
              " 'for': 0,\n",
              " 'man': 0,\n",
              " 'fire': 1,\n",
              " 'sat': 1,\n",
              " 'went': 0,\n",
              " 'the': 2,\n",
              " 'children': 1,\n",
              " 'around': 1,\n",
              " 'walk': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZBDayDt1hd"
      },
      "source": [
        "## 定義function:計算TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CinSM2ISt1he"
      },
      "source": [
        "def computeTF(wordDict, tokenize_item):\n",
        "    \"\"\"\n",
        "    wordDict : 文件內單詞對應出現數量的字典\n",
        "    tokenize_item : 文件tokenize後的輸出\n",
        "    \"\"\"\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(tokenize_item) ## tokenize_item單詞數量\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count/bagOfWordsCount ##單詞在該文件出現的次數/該文件擁有的所有單詞數量\n",
        "    return tfDict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufMNSmTft1he"
      },
      "source": [
        "## 定義function:計算IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKVRuUzt1hf"
      },
      "source": [
        "def computeIDF(documentsDict):\n",
        "    \"\"\"\n",
        "    documentsDict:為一個list，包含所有文件的wordDict\n",
        "    \"\"\"\n",
        "    import math\n",
        "    N = len(documentsDict)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documentsDict[0].keys(), 0)\n",
        "    for document in documentsDict:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1 ## 計算單詞在多少文件中出現過\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N/val) ## 計算IDF，Log (所有文件的數目/包含這個單詞的文件數目)\n",
        "    return idfDict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qNyFv6t1hf"
      },
      "source": [
        "## 定義function:計算TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rDHXexut1hg"
      },
      "source": [
        "def computeTFIDF(tf_item, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tf_item.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZLsTxet1hg"
      },
      "source": [
        "tfA = computeTF(numOfWordsA, tokenize_A)\n",
        "tfB = computeTF(numOfWordsB, tokenize_B)\n",
        "\n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "\n",
        "\n",
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLUqUoFLt1hh",
        "outputId": "718e53dd-5fc6-4262-ddc1-a5b7ad667c32"
      },
      "source": [
        "tfidfA"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0.09902102579427789,\n",
              " 'around': 0.0,\n",
              " 'children': 0.0,\n",
              " 'fire': 0.0,\n",
              " 'for': 0.09902102579427789,\n",
              " 'man': 0.09902102579427789,\n",
              " 'out': 0.09902102579427789,\n",
              " 'sat': 0.0,\n",
              " 'the': 0.0,\n",
              " 'walk': 0.09902102579427789,\n",
              " 'went': 0.09902102579427789}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZMLsCgVt1hh",
        "outputId": "9f4be04f-cf58-4422-bd78-713a55fcb14e"
      },
      "source": [
        "tfidfB"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0.0,\n",
              " 'around': 0.11552453009332421,\n",
              " 'children': 0.11552453009332421,\n",
              " 'fire': 0.11552453009332421,\n",
              " 'for': 0.0,\n",
              " 'man': 0.0,\n",
              " 'out': 0.0,\n",
              " 'sat': 0.11552453009332421,\n",
              " 'the': 0.0,\n",
              " 'walk': 0.0,\n",
              " 'went': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vux47p0bt1hi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}