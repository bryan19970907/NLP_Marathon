{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_情緒分析(作業)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9AMiZxP20fL",
        "outputId": "e9d0adcb-d535-4ce4-a338-24740a7cf229"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBW1QMHk625J",
        "outputId": "f1d886ec-7780-42e5-9545-bde0e720fba8"
      },
      "source": [
        "%cd './drive/My Drive/NLP/day36'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP/day36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDnlOHMo2z35"
      },
      "source": [
        "# 題目:電商產品評分文件以機器學習方式分辨是否為正向或負向\r\n",
        "#\r\n",
        "# 說明：輸入文件 positive.review 和 negative.review，兩者都是XML檔。我們用BeautifulSoup讀進來，\r\n",
        "# 擷取review_text，然後用NLTK自建Tokenizer。 先產生 word-to-index map 再產生 word-frequency vectors。\r\n",
        "# 之後 shuffle data 創造 train/test splits，留100個給 test 用。接著用Logistic Regression 分類器\r\n",
        "# 找出訓練組和測試組的準確度(Accuracy)。接著我們可以看看每個單字的正負權重，可以訂一個閥值，\r\n",
        "# 比方絕對值大於正負0.5，以確認情緒是顯著的。最後我們找出根據現有演算法歸類錯誤最嚴重的正向情緒和負向\r\n",
        "# 情緒的例子。\r\n",
        "#\r\n",
        "# 延伸:可用不同的tokenizer，不同的tokens_to_vector，不同的ML分類器做改進準確率的比較。最後可用您的\r\n",
        "# model去預測unlabeled.review檔的內容。\r\n",
        "#\r\n",
        "# 範例程式檔名: sentiment_情緒分析.py，以LogisticRegression 方式完成情緒分析。\r\n",
        "# 模組: sklearn, bs4, numpy, nltk\r\n",
        "# 輸入檔：stopwords.txt, /electronics 下 positive.review, negative.review\r\n",
        "# 成績：辨識百分率\r\n",
        "#\r\n",
        "#注意事項：nltk 需要有 punkt corpus 和 wordnet  資源\r\n",
        "#import nltk\r\n",
        "#nltk.download('punkt')\r\n",
        "#nltk.download('wordnet') \r\n",
        "#資料檔需在適當位置 jupyter 或 colab 才能看到，用colab時要上傳 data 到 ./sample_data 或 mount\r\n",
        "from __future__ import print_function, division\r\n",
        "from future.utils import iteritems\r\n",
        "from builtins import range"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mii4OG7Iwq"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlrlaKBN7QWd"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "# from http://www.lextek.com/manuals/onix/stopwords1.html\r\n",
        "stopwords = set(w.rstrip() for w in open('stopwords(作業數據).txt'))\r\n",
        "\r\n",
        "# 另一個 stopwords 的來源\r\n",
        "# from nltk.corpus import stopwords\r\n",
        "# stopwords.words('english')\r\n",
        "\r\n",
        "# 讀正向與負向 reviews\r\n",
        "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\r\n",
        "positive_reviews = BeautifulSoup(open('electronics/positive.review', encoding='utf-8').read(), features=\"html5lib\")\r\n",
        "positive_reviews = positive_reviews.findAll('review_text')\r\n",
        "\r\n",
        "negative_reviews = BeautifulSoup(open('electronics/negative.review', encoding='utf-8').read(), features=\"html5lib\")\r\n",
        "negative_reviews = negative_reviews.findAll('review_text')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kfWWNz08TEB",
        "outputId": "2a5a8d82-7f1b-415e-8c27-5ee387809b27"
      },
      "source": [
        "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhy6_KIC7U4A"
      },
      "source": [
        "# 基於nltk自建 tokenizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "\r\n",
        "def my_tokenizer(s):\r\n",
        "    s = s.lower() # downcase\r\n",
        "    tokens = nltk.word_tokenize(s) # 將字串改為tokens\r\n",
        "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\r\n",
        "    ######################\r\n",
        "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\r\n",
        "    pos = nltk.pos_tag(tokens)\r\n",
        "    #print(pos)\r\n",
        "    token_list = []\r\n",
        "    for t , p in (pos):\r\n",
        "      #print(t)\r\n",
        "      #print(p)\r\n",
        "      if p.startswith('J'):\r\n",
        "        pos = wordnet.ADJ\r\n",
        "      elif p.startswith('V'):\r\n",
        "        pos = wordnet.VERB\r\n",
        "      elif p.startswith('N'):\r\n",
        "        pos = wordnet.NOUN\r\n",
        "      elif p.startswith('R'):\r\n",
        "        pos = wordnet.ADV\r\n",
        "      else:\r\n",
        "        pos = wordnet.NOUN\r\n",
        "      tk = wordnet_lemmatizer.lemmatize(t,pos)\r\n",
        "      token_list.append(tk)\r\n",
        "    ######################\r\n",
        "    return token_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6h5hmdw8D7x",
        "outputId": "6b5d36f1-8b68-4631-de97-516f52e56d7b"
      },
      "source": [
        "# 先產生 word-to-index map 再產生 word-frequency vectors\r\n",
        "# 同時儲存 tokenized 版本未來不需再做 tokenization\r\n",
        "word_index_map = {}\r\n",
        "current_index = 0\r\n",
        "positive_tokenized = []\r\n",
        "negative_tokenized = []\r\n",
        "orig_reviews = []\r\n",
        "\r\n",
        "for review in positive_reviews:\r\n",
        "    orig_reviews.append(review.text)\r\n",
        "    tokens = my_tokenizer(review.text)\r\n",
        "    positive_tokenized.append(tokens)\r\n",
        "    for token in tokens:\r\n",
        "        if token not in word_index_map:\r\n",
        "            word_index_map[token] = current_index\r\n",
        "            current_index += 1\r\n",
        "\r\n",
        "for review in negative_reviews:\r\n",
        "    orig_reviews.append(review.text)\r\n",
        "    tokens = my_tokenizer(review.text)\r\n",
        "    negative_tokenized.append(tokens)\r\n",
        "    for token in tokens:\r\n",
        "        if token not in word_index_map:\r\n",
        "            word_index_map[token] = current_index\r\n",
        "            current_index += 1\r\n",
        "\r\n",
        "print(\"len(word_index_map):\", len(word_index_map))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(word_index_map): 10359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xaM6UGo8hEE"
      },
      "source": [
        "# now let's create our input matrices\r\n",
        "def tokens_to_vector(tokens, label):\r\n",
        "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\r\n",
        "    for t in tokens:\r\n",
        "        i = word_index_map[t]\r\n",
        "        x[i] += 1\r\n",
        "    x = x / x.sum() # 正規化數據提升未來準確度\r\n",
        "    x[-1] = label\r\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqp-KYpH8mZ7",
        "outputId": "c7322661-000d-4b64-a3aa-924e1268d0c7"
      },
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized)\r\n",
        "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\r\n",
        "data = np.zeros((N, len(word_index_map) + 1))\r\n",
        "i = 0\r\n",
        "for tokens in positive_tokenized:\r\n",
        "    xy = tokens_to_vector(tokens, 1)\r\n",
        "    data[i,:] = xy\r\n",
        "    i += 1\r\n",
        "\r\n",
        "for tokens in negative_tokenized:\r\n",
        "    xy = tokens_to_vector(tokens, 0)\r\n",
        "    data[i,:] = xy\r\n",
        "    i += 1\r\n",
        "\r\n",
        "# shuffle data 創造 train/test splits\r\n",
        "# 多次嘗試!\r\n",
        "orig_reviews, data = shuffle(orig_reviews, data)\r\n",
        "\r\n",
        "X = data[:,:-1]\r\n",
        "Y = data[:,-1]\r\n",
        "\r\n",
        "# 最後 100 列是測試用\r\n",
        "Xtrain = X[:-100,:]\r\n",
        "Ytrain = Y[:-100]\r\n",
        "Xtest = X[-100:,:]\r\n",
        "Ytest = Y[-100:]\r\n",
        "\r\n",
        "model = LogisticRegression()\r\n",
        "model.fit(Xtrain, Ytrain)\r\n",
        "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\r\n",
        "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.8\n",
            "Test accuracy: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGyqYKO0uB3J",
        "outputId": "1f2dd717-00a2-4387-d552-0ebd050804a0"
      },
      "source": [
        "# 列出每個字的正負 weight\r\n",
        "# 用不同的 threshold values!\r\n",
        "threshold = 0.5\r\n",
        "for word, index in iteritems(word_index_map):\r\n",
        "    weight = model.coef_[0][index]\r\n",
        "    if weight > threshold or weight < -threshold:\r\n",
        "        print(word, weight)\r\n",
        "\r\n",
        "\r\n",
        "# 找出歸類錯誤的例子\r\n",
        "preds = model.predict(X)\r\n",
        "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\r\n",
        "\r\n",
        "# 只列出最糟的\r\n",
        "minP_whenYis1 = 1\r\n",
        "maxP_whenYis0 = 0\r\n",
        "wrong_positive_review = None\r\n",
        "wrong_negative_review = None\r\n",
        "wrong_positive_prediction = None\r\n",
        "wrong_negative_prediction = None\r\n",
        "for i in range(N):\r\n",
        "    p = P[i]\r\n",
        "    y = Y[i]\r\n",
        "    if y == 1 and p < 0.5:\r\n",
        "        if p < minP_whenYis1:\r\n",
        "            wrong_positive_review = orig_reviews[i]\r\n",
        "            wrong_positive_prediction = preds[i]\r\n",
        "            minP_whenYis1 = p\r\n",
        "    elif y == 0 and p > 0.5:\r\n",
        "        if p > maxP_whenYis0:\r\n",
        "            wrong_negative_review = orig_reviews[i]\r\n",
        "            wrong_negative_prediction = preds[i]\r\n",
        "            maxP_whenYis0 = p\r\n",
        "\r\n",
        "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\r\n",
        "print(wrong_positive_review)\r\n",
        "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\r\n",
        "print(wrong_negative_review)\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unit -0.6695902475537121\n",
            "bad -1.2481254268513986\n",
            "cable 0.7097135573309317\n",
            "time -0.7797474136737502\n",
            "'ve 0.7437868665377392\n",
            "month -0.7427275559666142\n",
            "space 0.5868253023596195\n",
            "sound 1.053253078112549\n",
            "lot 0.6183486253048527\n",
            "you 1.0756818397978483\n",
            "n't -2.135891083167606\n",
            "easy 1.9194427409322823\n",
            "tell -0.6474416190286278\n",
            "quality 1.6629773795321274\n",
            "company -0.5061803005834855\n",
            "card -0.6386451610504666\n",
            "item -1.0247401688881477\n",
            "perfect 1.0624275414848718\n",
            "fast 0.9688355210458411\n",
            "price 2.7151396416058664\n",
            "value 0.5504378949801975\n",
            "money -0.9966783213043319\n",
            "memory 0.8299065920444407\n",
            "picture 0.5568070443042795\n",
            "bit 0.6099061917458853\n",
            "happy 0.5600758027219361\n",
            "travel 0.5131140400920169\n",
            "pretty 0.7753255277758027\n",
            "highly 0.9821763949497351\n",
            "recommend 0.7605609117761258\n",
            "customer -0.6855856685177918\n",
            "support -0.8976699038310081\n",
            "little 0.8594319128437713\n",
            "stop -0.84822292755502\n",
            "return -2.4167595421262513\n",
            "excellent 1.2219326396216272\n",
            "love 1.0925224705627148\n",
            "feature 0.5283015619986563\n",
            "software -0.5152058758678231\n",
            "home 0.5195142265132077\n",
            "useless -0.5034188759121727\n",
            "week -0.7109133945184671\n",
            "size 0.5222263521920465\n",
            "use 0.5370996006837202\n",
            "break -0.5922965164464805\n",
            "video 0.5838070815740813\n",
            "poor -0.7449488989463288\n",
            "look 0.8508927858133284\n",
            "expect 0.5075478701233351\n",
            "then -1.090727710636588\n",
            "call -0.6487334901409979\n",
            "try -1.4233661057921008\n",
            "start -0.6227226145672421\n",
            "static -0.526112110012127\n",
            "radio -0.5213537582236626\n",
            "comfortable 0.6908378295038075\n",
            "hour -0.5709049469874475\n",
            "send -1.0712010016130369\n",
            "speaker 0.8441586826853292\n",
            "warranty -0.6235544947518277\n",
            "broken -0.5107828654662885\n",
            "junk -0.5357401762001247\n",
            "waste -1.1483763818138641\n",
            "paper 0.6368288979334978\n",
            "refund -0.7193524674933908\n",
            "Most wrong positive review (prob = 0.35946917910353554, pred = 0.0):\n",
            "\n",
            "A device like this either works or it doesn't.  This one happens to work\n",
            "\n",
            "Most wrong negative review (prob = 0.6207018596839823, pred = 1.0):\n",
            "\n",
            "I like the HP 96 because it can be used in many HP printers that use the HP 98,  but its a better deal than the HP 98 because it contains almost twice as much ink. \n",
            "\n",
            "HP's ink cartridges provide excellent performance,  but I think they are over priced \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}