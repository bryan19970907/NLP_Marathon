{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day20_21-KNN實作作業.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S"
      },
      "source": [
        "# K-Nearest Neighbors (K-NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trDZkUpzs5lX"
      },
      "source": [
        "### 參考課程實作並在datasets_483_982_spam.csv的資料集中獲得90% 以上的 accuracy (testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbwg-RvluTFW",
        "outputId": "cfebd74f-64e2-4638-e79f-b08593dcc753"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj-sBl4cuYvS",
        "outputId": "a2d206a9-57a3-41c6-93c7-e01c3d592845"
      },
      "source": [
        "%cd './drive/My Drive/NLP/day20'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP/day20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "UxyBN0rWs5lZ",
        "outputId": "8fe3c334-1015-4191-9890-c3e2d80bcc74"
      },
      "source": [
        "dataset = pd.read_csv(r'datasets_483_982_spam.csv', encoding = 'latin-1' , usecols=[0,1])\n",
        "\n",
        "\"讀取資料集\"\n",
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1                                                 v2\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUJ1qTFN2EBx",
        "outputId": "a294e208-397e-44ba-ad99-29507dfd52e8"
      },
      "source": [
        "print(len(dataset))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clh4QgczvjL8"
      },
      "source": [
        "all_data=[]\r\n",
        "\r\n",
        "for i in range(len(dataset)):\r\n",
        "  if (dataset.iloc[i,0] == 'ham'):\r\n",
        "    is_spam = 0\r\n",
        "  else:\r\n",
        "    is_spam = 1\r\n",
        "  \r\n",
        "  content = dataset.iloc[i,1].strip()\r\n",
        "  all_data.append([content , is_spam])\r\n",
        "\r\n",
        "all_data = np.array(all_data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilv9QF-2s5la"
      },
      "source": [
        "### 取出訓練內文與標註"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFd_kgc3s5la"
      },
      "source": [
        "X = all_data[:,0]\n",
        "Y = all_data[:,1].astype(np.uint8)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZCs0Aj4s5lb",
        "outputId": "9154134c-127d-4991-b128-c2b7c81107d1"
      },
      "source": [
        "print('Training Data Examples : \\n{}'.format(X[:5]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data Examples : \n",
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
            " 'Ok lar... Joking wif u oni...'\n",
            " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
            " 'U dun say so early hor... U c already then say...'\n",
            " \"Nah I don't think he goes to usf, he lives around here though\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3E1imf6s5lc",
        "outputId": "c35e2530-c965-446a-a984-e4e1bde4acce"
      },
      "source": [
        "print('Labeling Data Examples : \\n{}'.format(Y[:5]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labeling Data Examples : \n",
            "[0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYMDa8vs5lc"
      },
      "source": [
        "### 文字預處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjW-VBTDs5ld",
        "outputId": "9445390f-a236-4648-ad8a-214ec4eafcf0"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download(['stopwords','punkt','averaged_perceptron_tagger','wordnet'])\n",
        "\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "  pos = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = { \"J\": wordnet.ADJ,\n",
        "          \"N\": wordnet.NOUN,\n",
        "          \"V\": wordnet.VERB,\n",
        "          \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(pos , wordnet.NOUN)\n",
        "\n",
        "\n",
        "def clean_content(X):\n",
        "  # remove non-alphabet characters\n",
        "  X_clean = [re.sub('[^a-zA-Z]',' ', x).lower() for x in X]\n",
        "  # tokenize\n",
        "  X_word_tokenize = [nltk.word_tokenize(x) for x in X_clean]\n",
        "  # stopwords & lemmatize\n",
        "  X_stopwords_lemmatizer = []\n",
        "  \n",
        "  for word_list in X_word_tokenize:\n",
        "    content_clean = []\n",
        "    \n",
        "    for word in word_list:\n",
        "      if word not in set(stopwords.words('english')):\n",
        "        word = lemmatizer.lemmatize(word , get_wordnet_pos(word))\n",
        "        content_clean.append(word)\n",
        "\n",
        "    X_stopwords_lemmatizer.append(content_clean)\n",
        "    \n",
        "  X_output = [' '.join(x) for x in X_stopwords_lemmatizer]\n",
        "  return X_output\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9kYf0pHs5ld"
      },
      "source": [
        "X = clean_content(X)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPwTpeCIs5ld"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W--02hqhs5le"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#max_features是要建造幾個column，會按造字出現的高低去篩選 \n",
        "cv=CountVectorizer(max_features = 1500)\n",
        "X=cv.fit_transform(X).toarray()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GKQ8LuNs5le",
        "outputId": "7755b4cb-3b00-408c-9197-eec57d7323ea"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 1500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP"
      },
      "source": [
        "## Training the K-NN model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0pFVAmciHQs",
        "outputId": "be5a79c7-a3b7-4a03-82ab-507cadfa051b"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyxW5b395mR2"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8YOXsQy58rP",
        "outputId": "137f8199-5a8e-4bcb-a1ed-64fed003345a"
      },
      "source": [
        "print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainset Accuracy: 0.942562261610949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcbA2tfVs5lj",
        "outputId": "6bcf1cb9-9c30-4e8e-f679-1e8a9c988389"
      },
      "source": [
        "print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testset Accuracy: 0.9147982062780269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYVQH-l5NpE"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMTb2O4hwM"
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Hwj34ziWQW"
      },
      "source": [
        "## Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6bpZwUiiXic",
        "outputId": "121c74ba-0b24-4d46-fa55-de1f040ba315"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[949   0]\n",
            " [ 95  71]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9147982062780269"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}